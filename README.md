[<img width="600" alt="image" src="https://github.com/boostcampaitech5/level3_nlp_finalproject-nlp-03/assets/75467530/37831d49-2e42-46ca-bcae-6f9caeaa934b">](https://boostcamp.connect.or.kr/)

# :honey_pot:ğŸ¯:NELLM(ë‚¼ë¦„): NEgotiation Large Language Model
[![Static Badge](https://img.shields.io/badge/-Notion-white?logo=Notion&logoColor=black)](https://www.notion.so/boostcampait/NLP-03-NELLM-54aea5571d5f488f96cf8668fe5a8b80?pvs=4)
[![Static Badge](https://img.shields.io/badge/-Youtube-red?logo=Youtube)](https://www.youtube.com/watch?v=66qdusEwjZw)
[![Static Badge](https://img.shields.io/badge/%F0%9F%A4%97-Huggingface-yellow)](https://huggingface.co/ggul-tiger)

NELLM(ë‚¼ë¦„)ì€ ì¤‘ê³ ê±°ë˜ì—ì„œ íŒë§¤ì ëŒ€ì‹  ê°€ê²©ì„ í˜‘ìƒì— 1ëŒ€ nìœ¼ë¡œ ëŒ€ì‘í•˜ëŠ” ì±—ë´‡ ì–´í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.

# Demo & Example
<h3 align="center"><a href=http://nellm.site> http://nellm.site </a></h3>

<img width="935" alt="image" src="imgs/demo.png">

- ë°°í¬ ê¸°ê°„ : 23.07.19 21:00 ~ 23.08.18 16:00
- ë°°í¬ í˜•íƒœ : ë§ì€ userë“¤ì´ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê¸° ìœ„í•´ NELLMê³¼ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ê²Œì„ í˜•íƒœë¡œ ë°°í¬í–ˆìŠµë‹ˆë‹¤.
- ì´ìš© ë°©ë²• : `íšŒì› ë“±ë¡`í›„ `íŒë§¤ ëª©ë¡`ì—ì„œ ì›í•˜ëŠ” ìƒí’ˆ í˜ì´ì§€ì— ì…ì¥í•˜ì—¬ NELLMê³¼ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# ğŸŒ±Members

|<img src='https://avatars.githubusercontent.com/u/110003154?v=4' height=100 width=100px></img>|<img src='https://avatars.githubusercontent.com/u/60145579?v=4' height=100 width=100px></img>|<img src='https://avatars.githubusercontent.com/u/54995090?v=4' height=100 width=100px></img>|<img src='https://avatars.githubusercontent.com/u/75467530?v=4' height=100 width=100px></img>|<img src='https://avatars.githubusercontent.com/u/65614582?v=4' height=100 width=100px></img>|
| --- | --- | --- | --- | --- |
| [ê¹€ë¯¼í˜](https://github.com/torchtorchkimtorch) | [ê¹€ì˜ì§„](https://github.com/KimuGenie) | [ê¹€ì„±ìš°](https://github.com/tjddn0402) | [ì˜¤ì›íƒ](https://github.com/dnjdsxor21) | [ì •ì„¸ì—°](https://github.com/jjsyeon) |
|Data|Model|Model|Service|Service|
|- Data ìˆ˜ì§‘ Â· ë²ˆì—­<br>- ChatGPT API í™œìš© ë°ì´í„° ìƒì„±<br>- ë°ì´í„° ì „ì²˜ë¦¬ Â· EDA|- QLoRA Â· Vicuna í•™ìŠµ êµ¬í˜„<br>- ëª¨ë¸ ì¶œë ¥ ì¡°ì ˆ Advisor ì œì‘<br>- ë°ì´í„° ê²€ìˆ˜|- LoRA fine-tuning<br>- ë°ì´í„° ê²€ìˆ˜|- Web API, Model API ì„œë¹™ ë° ê´€ë¦¬<br>- ì‚¬ìš©ì ë°ì´í„°, ë¡œê·¸ ë¶„ì„|- ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìë™í™”<br>- ì‚¬ìš©ì ë°ì´í„° ë¶„ì„ ë° í•™ìŠµ ë°ì´í„° ì œì‘|


# Model

ğŸ¤—[NELLM(ë‚¼ë¦„)](https://huggingface.co/ggul-tiger)ì€ [KULLM(êµ¬ë¦„)](https://github.com/nlpai-lab/KULLM)ì„ ë°”íƒ•ìœ¼ë¡œ QLoRA fine-tuningëœ ëª¨ë¸ì…ë‹ˆë‹¤.

## Key Features
1. [QLoRA fine-tuning](./chat_bot/scripts/train.py)  
<img src="imgs/model_architecture.png">
fp16ì—ì„œ int8ë¡œ quantizingí•œ í›„ LoRA(Low Rank Adaptation)ì„ ì ìš©í•˜ì—¬ ê°€ìš©í•œ ìì› (NVIDIA V100 VRAM 32GB) ë‚´ì—ì„œ í•™ìŠµì´ ê°€ëŠ¥í•˜ê²Œ í•˜ì˜€ë‹¤.  
2. [Advisor](./chat_bot/neural_chat/advisor.py)  
<img src="imgs/advisor.png">
ê°€ê²©ì„ regexë¡œ ì¶”ì í•˜ë©° [ruleì„ ê¸°ë°˜](./chat_bot/neural_chat/price_parser.py)ìœ¼ë¡œ NELLMì˜ ë°œí™”ë¥¼ ì¼ì •ë¶€ë¶„ ê°•ì œí•˜ì—¬ controlí•˜ì˜€ìŠµë‹ˆë‹¤.
3. [Vicuna Training](https://lmsys.org/blog/2023-03-30-vicuna/)  
<img src="imgs/vicuna.png">
[íŒë§¤ìì˜ ë°œí™”ë§Œ í•™ìŠµí•˜ë„ë¡ ë°ì´í„°ì…‹ì„ êµ¬ì¶•(./chat_bot/neural_chat/dataset/e2e_dataset.py)](./chat_bot/neural_chat/dataset/e2e_dataset.py)í•˜ì—¬ ëª¨ë¸ì´ êµ¬ë§¤ìì˜ ë°œí™”ê¹Œì§€ í˜¼ë™í•˜ì—¬ í•¨ê»˜ ìƒì„±í•˜ëŠ” í˜„ìƒì„ ë°©ì§€í–ˆìŠµë‹ˆë‹¤.

# Dataset

ğŸ¤—[NELLM ë°ì´í„°ì…‹ v1](https://huggingface.co/datasets/ggul-tiger/negobot_cleaned_100): ChatGPTë¡œ ìì²´ ìƒì„±í•œ ë°ì´í„°ì…‹  
ğŸ¤—[NELLM ë°ì´í„°ì…‹ v2](https://huggingface.co/datasets/ggul-tiger/negobot_cleaned_361): v1 + ì œëª©, ìƒí’ˆ ì„¤ëª…, ê°€ê²© í¬ë¡¤ë§ í›„ ì±„íŒ…ë§Œ ChatGPTë¡œ ìƒì„±í•œ ë°ì´í„°ì…‹  
ğŸ¤—[NELLM ë°ì´í„°ì…‹ v3](https://huggingface.co/datasets/ggul-tiger/negobot_361_weakcase_injected): v2ì— ìœ ì € ë¡œê·¸ì—ì„œ íŠ¹ì´ ì¼€ì´ìŠ¤(ì˜ˆ: ë¬´ë£Œ ë‚˜ëˆ” í•´ì£¼ì„¸ìš”.)ì— ëŒ€ì‘í•˜ëŠ” ë°œí™”ê°€ ì¶”ê°€ëœ ë°ì´í„°ì…‹  
ğŸ¤—[ìœ ì € ë°ì´í„°ì…‹](https://huggingface.co/datasets/ggul-tiger/negobot_userdata): ì•± ë°°í¬ í›„ ì‚¬ìš©ìë¡œë¶€í„° ì–»ì–´ ì •ì œí•œ ë°ì´í„°ì…‹  

## Data Schema
```json
{
    "title":"{ì œëª©}",               // str
    "description":"{ìƒí’ˆ ì •ë³´}",    // str
    "price": "ìƒí’ˆ ê°€ê²©",           // int
    "result":"ACCEPT ë˜ëŠ” DENY",    // str
    "events":[
        {"role":"êµ¬ë§¤ì","message":"ì•ˆë…•í•˜ì„¸ìš”! ë¬¼ê±´ íŒ”ë ¸ë‚˜ìš”?"},
        {"role":"íŒë§¤ì", "message":"ì•„ì§ ì•ˆíŒ”ë ¸ìŠµë‹ˆë‹¤~"},
        //...
        {"role":"êµ¬ë§¤ì","message":"##<{ìµœì¢… ì œì•ˆ ê°€ê²©}>##"},
        {"role":"íŒë§¤ì", "message":"##<{ìˆ˜ë½/ê±°ì ˆ}>##"},
    ],
}
```
ì •ì˜ëœ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ëˆ„ì–´ì§„ë‹¤. `events`ì˜ ê¸¸ì´ê°€ ë°ì´í„°ë§ˆë‹¤ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— json-like dataë¡œ ì •ì˜í•˜ì˜€ë‹¤.
- `title`: str
    - ì¤‘ê³ ê±°ë˜ íŒë§¤ê¸€ ì œëª©
- `description`: str
    - ì¤‘ê³ ê±°ë˜ íŒë§¤ê¸€ ë‚´ìš©
- `price`: int
    - íŒë§¤ìê°€ ì˜¬ë¦° ê°€ê²©
- `result`: str
    - ê±°ë˜ê°€ ì„±ì‚¬ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ì˜ë¯¸í•œë‹¤. `"##<ìˆ˜ë½>##"` ë˜ëŠ” `"##<ê±°ì ˆ>##"`ì´ë‹¤.
- `events`: List[Dict]
    - `role`ê³¼ `message`ë¥¼ key ê°’ìœ¼ë¡œ ê°€ì§€ëŠ” dictionaryë“¤ì„ ì›ì†Œë¡œ ê°€ì§€ê³ , ëŒ€í™” í„´ì˜ ê¸¸ì´ëŠ” ë°ì´í„°ë§ˆë‹¤ ë‹¤ë¥´ë‹¤.

## Synthetic Data Generation
![ChatGPT](https://img.shields.io/badge/chatGPT-74aa9c?style=for-the-badge&logo=openai&logoColor=white)

ê¸°ì¡´ ì˜ì–´ ë°ì´í„°ì…‹ì¸ [CraigslistBargain](https://github.com/stanfordnlp/cocoa)ë¥¼ ë²ˆì—­í•˜ì—¬ ì‚¬ìš©í–ˆìœ¼ë‚˜, ë‹¨ìˆœí•œ ëŒ€í™”íŒ¨í„´, ë²ˆì—­ì²´, ë¬¸í™” ì°¨ì´ì— ë”°ë¥¸ ë¶€ì ì ˆí•œ ë‚´ìš© ë“±ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì ChatGPT APIë¥¼ ì´ìš©í•´ ë°ì´í„°ë¥¼ ì§ì ‘ ìƒì„±í–ˆë‹¤.
### [Prompt Rules](./dataprincess.ipynb)
1. êµ­ë‚´ ì¤‘ê³ ê±°ë˜ í”Œë«í¼ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìš©ì–´ ë°˜ì˜ (ex. ë„¤ê³ , ì¿¨ê±°ë˜ ë“±)
2. ëŒ€í™” íŒ¨í„´ ë‹¤ì–‘í™” 
    - êµ¬ë§¤ìì˜ í˜ë¥´ì†Œë‚˜ ë¶€ì—¬
    - ê·¸ì— ë”°ë¥¸ íŒë§¤ìì˜ ë°˜ì‘
    - ê°€ê²© ì œì•ˆì˜ ê·¼ê±°ë¡œ ìƒí’ˆ ì„¤ëª… í™œìš©
3. ê±°ë˜ê°€ ì„±ì‚¬(ACCEPT)ë˜ê¸°ë§Œ í•˜ëŠ” ë°ì´í„° í¸ì¤‘ì„ ë§‰ê¸° ìœ„í•´ 30% í™•ë¥ ë¡œ ê±°ì ˆ(DENY)í•˜ëŠ” ë°ì´í„°ë¥¼ ìƒì„±í•˜ë„ë¡ prompt êµ¬ì„±

# Web
![HTML5](https://img.shields.io/badge/html5-%23E34F26.svg?style=for-the-badge&logo=html5&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)
![SQLite](https://img.shields.io/badge/sqlite-%2307405e.svg?style=for-the-badge&logo=sqlite&logoColor=white)
![MongoDB](https://img.shields.io/badge/MongoDB-%234ea94b.svg?style=for-the-badge&logo=mongodb&logoColor=white)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)
![Google Cloud](https://img.shields.io/badge/GoogleCloud-%234285F4.svg?style=for-the-badge&logo=google-cloud&logoColor=white)
![ì„œë¹„ìŠ¤ êµ¬ì¡°ë„](imgs/service_architecture.png)
- Frontend : HTML5
- Backend : FastAPI
- App server : GCP (Google Cloud Platform)
- Model Server : Upstageì—ì„œ ì œê³µë°›ì€ V100 ì„œë²„
- DB : ì±„íŒ… ë°ì´í„°ëŠ” App Serverì˜ SQLite(Relational DB)ì— ì €ì¥ë˜ë©°, 24ì‹œê°„ë§ˆë‹¤ í•œ ë²ˆì”© json í˜•íƒœì˜ dataë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ MongoDBë¡œ ì˜®ê²¨ì§„ë‹¤.


# Environment
- Ubuntu 18.04.6 LTS
- NVIDIA Volta V100 VRAM 32GB
- Python>=3.9
- Poetry & Pyenv ê°€ìƒí™˜ê²½ ì„¤ì •  
    poetryê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ í›„, ì„¤ì¹˜ë˜ì–´ìˆì§€ ì•Šë‹¤ë©´ [ì„¤ì¹˜](https://python-poetry.org/docs/#installation)í•œë‹¤.
    ```bash
    curl -sSL https://install.python-poetry.org | python3 -
    ```
    ì´ repoë¥¼ clone í•œë‹¤.
    ```bash
    git clone https://github.com/boostcampaitech5/level3_nlp_finalproject-nlp-03
    cd level3_nlp_finalproject-nlp-03
    poetry install
    ```
- [Web server í™˜ê²½ ì„¤ì •](./app/README.md)

# How to run

## Model train & evaluation
### 1. LoRA fine-tuning
```bash
> python chat_bot/scripts/train.py --help

usage: train.py [-h] [--train-dataset-names TRAIN_DATASET_NAMES [TRAIN_DATASET_NAMES ...]] [--model-name-or-checkpoint MODEL_NAME_OR_CHECKPOINT] [--dataset-type DATASET_TYPE] [--conv-template CONV_TEMPLATE] [--max-length MAX_LENGTH] [--epoch EPOCH]
                        [--max-steps MAX_STEPS] [--batch-size BATCH_SIZE] [--grad-accum GRAD_ACCUM] [--lr LR] [--output-dir OUTPUT_DIR] [--run-name RUN_NAME] [--peft-type PEFT_TYPE] [--lora-r LORA_R] [--lora-alpha LORA_ALPHA] [--lora-dropout LORA_DROPOUT]
                        [--n_virtual_token N_VIRTUAL_TOKEN]

optional arguments:
  -h, --help            show this help message and exit
  --train-dataset-names TRAIN_DATASET_NAMES [TRAIN_DATASET_NAMES ...]
                        list of dataset names. use as --train-dataset-names ds1 ds2
  --model-name-or-checkpoint MODEL_NAME_OR_CHECKPOINT
  --dataset-type DATASET_TYPE
  --conv-template CONV_TEMPLATE
  --max-length MAX_LENGTH
  --epoch EPOCH
  --max-steps MAX_STEPS
  --batch-size BATCH_SIZE
  --grad-accum GRAD_ACCUM
  --lr LR
  --output-dir OUTPUT_DIR
  --run-name RUN_NAME
  --peft-type PEFT_TYPE
  --lora-r LORA_R
  --lora-alpha LORA_ALPHA
  --lora-dropout LORA_DROPOUT
  --n_virtual_token N_VIRTUAL_TOKEN
```
- ì‹¤í–‰ ì˜ˆì‹œ
```bash
python chat_bot/scripts/train.py \
    --train-dataset-names ggul-tiger/{dataset_name_1} ggul-tiger/{dataset_name_2}
```

### 2. evaluation
```bash
> python chat_bot/scripts/eval.py --help

usage: eval.py [-h] --data-path DATA_PATH --model_checkpoint_path MODEL_CHECKPOINT_PATH [--conv-template-name CONV_TEMPLATE_NAME] [--num-rollouts NUM_ROLLOUTS]

optional arguments:
  -h, --help            show this help message and exit
  --data-path DATA_PATH
  --model_checkpoint_path MODEL_CHECKPOINT_PATH
  --conv-template-name CONV_TEMPLATE_NAME
  --num-rollouts NUM_ROLLOUTS
```
- ì‹¤í–‰ ì˜ˆì‹œ
```bash
python chat_bot/scripts/eval.py \
    --data-path ./data/sample_data.json \
    --model_checkpoint_path ggul-tiger/{model_name}
```

## Web server
### 1. WebAPI: NELLMì˜ í”„ë¡ íŠ¸ & ë°±ì—”ë“œë¥¼ êµ¬ì„±í•˜ëŠ” API
```bash
cd app
uvicorn main:app --port 80
```
### 2. ModelAPI: êµ¬ë§¤ìì˜ ì±„íŒ…ì„ ì…ë ¥ ë°›ì•„ ì ì ˆí•œ ëŒ€ë‹µì„ ì¶œë ¥í•˜ëŠ” API
```bash
cd modelapi
uvicorn main:app --port 30007       
```

# References
- Verma, Siddharth, et al. â€œCHAI: A CHatbot AI for Task-Oriented Dialogue with Offline Reinforcement Learning.â€ Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2022.
- Lab, NLP &. AI, and Human-Inspired AI research. KULLM: Korea University Large Language Model Project. GitHub, 2023, https://github.com/nlpai-lab/kullm.
- He, He, et al. â€œDecoupling Strategy and Generation in Negotiation Dialogues.â€ CoRR, vol. abs/1808.09637, 2018, http://arxiv.org/abs/1808.09637.
- Ko, Hyunwoong, et al. A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models. 2023.
- Dettmers, Tim, et al. "Qlora: Efficient finetuning of quantized llms." arXiv preprint arXiv:2305.14314 (2023).
- @misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
